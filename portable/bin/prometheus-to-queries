#!/usr/bin/env -S python-venv --virtualenv dotfiles
import json
import logging
import re
import sys
from dataclasses import dataclass
from typing import TextIO

from jmullan.cmd import cmd
from jmullan.logging import easy_logging

logger = logging.getLogger(__name__)

HELP = "# HELP"
TYPE = "# TYPE"
FILTER_REGEX_Q = '^([^=]+)(=|!=|=~|!~)"([^"]+)",?(.*)'
FILTER_REGEX = "^([^=]+)(=|!=|=~|!~)([^,]+),?(.*)"
LABEL_REGEX = r'^\s*([^=]+)="([^"]+)",?(.*)'

value_escapes = {
    "\n": "\\n",
    "\t": "\\t",
    "\\": "\\\\",
}


@dataclass
class Filter:
    key: str
    operator: str
    value: str


def parse_raw_filter(line: str) -> dict[str, Filter]:
    if empty(line):
        return {}
    line = line.strip()
    if line.startswith("{") and line.endswith("}"):
        line = line.removeprefix("{")
        line = line.removesuffix("}")
        return parse_raw_filter(line)
    pairs = {}
    while len(line) > 3:
        line = line.strip()
        # x=y is the minimum filter length!
        match = re.match(FILTER_REGEX_Q, line)
        if not match:
            match = re.match(FILTER_REGEX, line)
        if not match:
            break

        key = match.group(1).strip()
        operator = match.group(2).strip()
        value = match.group(3).strip()
        pairs[key] = Filter(key, operator, value)
        line = match.group(4).strip()
    return pairs


def parse_raw_filters(raw_filters: list[str | None] | None) -> dict[str, Filter]:
    if not raw_filters:
        return {}
    filters = {}
    for raw_filter in raw_filters:
        filters.update(parse_raw_filter(raw_filter))
    return filters


def make_filter_string(filters: dict[str, Filter] | None) -> str:
    # {project=~"$project", env=~"$env"}
    if not filters:
        return ""
    parts = []
    for f in filters.values():
        parts.append(f'{f.key.strip()}{f.operator}"{f.value}"')
    if parts:
        return "{" + ", ".join(parts) + "}"
    else:
        return ""


def empty(x: str | None) -> bool:
    return x is None or len(x.strip()) == 0


def strip(x: str | None) -> str | None:
    return x.strip() if x is not None else None


def twain(x: str | None) -> tuple[str | None, str | None]:
    if empty(x):
        return None, None
    x = x.strip()
    parts = x.split(" ", 1)
    if not parts or len(parts) == 1:
        return None, None
    stat_name = parts[0].strip()
    stat_help = parts[1].strip()
    if empty(stat_name) or empty(stat_help):
        return None, None
    return parts[0], parts[1]


def get_help(line: str) -> tuple[str | None, str | None]:
    return twain(line)


def get_stat_type(line: str) -> tuple[str | None, str | None]:
    return twain(line)


def parse_labels(line: str) -> set[str] | None:
    if empty(line):
        return None
    line = line.strip()
    if not line.startswith("{") or not line.endswith("}"):
        return None
    line = line.removeprefix("{")
    line = line.removesuffix("}")
    if empty(line):
        return None
    pairs = {}
    while match := re.match(LABEL_REGEX, line):
        key = match.group(1)
        value = match.group(2)
        pairs[key] = value
        line = match.group(3)
    return set(pairs.keys())


def get_stat(line: str) -> tuple[str | None, dict[str, str] | None]:
    matches = re.match(r"^\s*([^{]+)\s*({[^[}]+})\s*$", line.strip())
    if matches:
        stat = matches.group(1)
        labels = parse_labels(matches.group(2))
        if stat:
            return stat, labels

    stat, _ = twain(line)
    if stat is None:
        return None, None
    curly_location = stat.find("{")
    if curly_location == -1:
        return None, None
    elif curly_location == 0:
        return stat, None
    else:
        stat_name = stat[:curly_location]
        labels = stat[curly_location:]
        return stat_name, parse_labels(labels)


def update_dict_if_not_set(from_dict: dict[str, str], into_dict: dict[str, str]):
    for key, value in from_dict.items():
        if key not in into_dict:
            into_dict[key] = value


def guess_stat_type(stat_name, stat_types):
    stat_type = stat_types.get(stat_name)
    if stat_type is not None:
        return stat_type
    if stat_name.endswith("_count") or stat_name.endswith("_total"):
        return "counter"
    return "gauge"


def expand_suffixes(stat_name: str, suffixes: list[str], value: str) -> dict[str, str]:
    derived_stat_values = {}
    for suffix in suffixes:
        derived_stat_values[f"{stat_name}_{suffix}"] = value
    return derived_stat_values


def get_dimensions(labels: dict) -> str | None:
    if not labels:
        return None
    return " ".join("{{ %s }}" % k for k in labels.keys())


def prometheus_to_queries(
    contents: str,
    filters: dict[str, Filter] | None = None,
    as_grafana: bool = False,
    grafana_first_letter: str = "A"
) -> str | None:
    if filters is None:
        filters = dict()
    filter_string = make_filter_string(filters)
    stat_names = set()
    stat_helps = {}
    stat_types = {}
    stat_labels: dict[str, dict] = {}
    summaries = set()
    for line in contents.split("\n"):
        line = strip(line)
        if empty(line):
            continue
        if line.startswith(HELP):
            stat_name, stat_help = get_help(line.removeprefix(HELP))
            if empty(stat_name) or empty(stat_help):
                continue
            stat_helps[stat_name] = stat_help
        elif line.startswith(TYPE):
            stat_name, stat_type = get_stat_type(line.removeprefix(TYPE))
            if empty(stat_name) or empty(stat_type):
                continue
            stat_types[stat_name] = stat_type
            if "histogram" == stat_type:
                sub_types = expand_suffixes(
                    stat_name, ["bucket", "count", "sum"], "counter"
                )
                update_dict_if_not_set(sub_types, stat_types)
            elif "summary" == stat_type:
                sub_types = expand_suffixes(stat_name, ["count", "sum"], "counter")
                update_dict_if_not_set(sub_types, stat_types)
                summaries.add(stat_name)
        else:
            stat_name, labels = get_stat(line)
            if empty(stat_name):
                continue
            stat_names.add(stat_name)
            if labels:
                if stat_name not in stat_labels:
                    stat_labels[stat_name] = dict()
                for label in labels:
                    stat_labels[stat_name][label] = None
    for stat_name, stat_type in stat_types.items():
        if "histogram" == stat_type:
            sub_helps = expand_suffixes(
                stat_name, ["bucket", "count", "sum"], stat_helps.get(stat_name)
            )
            update_dict_if_not_set(sub_helps, stat_helps)
        elif "summary" == stat_type:
            sub_helps = expand_suffixes(
                stat_name, ["count", "sum"], stat_helps.get(stat_name)
            )
            update_dict_if_not_set(sub_helps, stat_helps)
    if not as_grafana:
        return dump_list(stat_names, stat_types, filters, stat_labels, filter_string, summaries)
    else:
        return dump_grafana(stat_names, stat_types, filters, stat_labels, filter_string, summaries, grafana_first_letter)


def dump_list(stat_names: set[str], stat_types: dict[str, str], filters: dict[str, dict[str, str]], stat_labels: dict[str, dict[str, str]], filter_string: str, summaries: set[str]):
    output = []
    # sum(genie_archive_item_priority{project=~"$project", env=~"$env"}) by (project, env, lag_project, item_type, queued_priority) > 0
    stat_defs = {}
    for stat_name in sorted(stat_names):
        stat_type = guess_stat_type(stat_name, stat_types)
        labels = dict()
        labels.update(filters or dict())
        labels.update(stat_labels.get(stat_name) or dict())
        name_filtered = f"{stat_name}{filter_string}"

        if "counter" == stat_type:
            name_filtered = f"rate({name_filtered}[$__rate_interval])"
        if labels:
            label_string = ", ".join(labels.keys())
            if "max" in name_filtered:
                stat_def = f"max by ({label_string}) ({name_filtered})"
            else:
                stat_def = f"sum by ({label_string}) ({name_filtered})"
        else:
            stat_def = f"{name_filtered}"
        dimensions = get_dimensions(labels)
        if dimensions:
            as_legend = f" AS {stat_name} {dimensions}"
        else:
            as_legend = stat_name
        output.append(f"{stat_def}{as_legend}")
        stat_defs[stat_name] = stat_def
    for summary in summaries:
        seconds_def = stat_defs.get(f"{summary}_sum")
        count_def = stat_defs.get(f"{summary}_count")
        if seconds_def is not None and count_def is not None:
            output.append(f"{seconds_def} / {count_def}")
    return "\n".join(output)


def dump_grafana(
    stat_names: set[str],
    stat_types: dict[str, str],
    filters: dict[str, dict[str, str]],
    stat_labels: dict[str, dict[str, str]],
    filter_string: str,
    summaries: set[str],
    grafana_first_letter: str
):
    targets = []
    # sum(genie_archive_item_priority{project=~"$project", env=~"$env"}) by (project, env, lag_project, item_type, queued_priority) > 0
    ref_int = ord(grafana_first_letter) - ord('A')
    stat_defs = {}
    for stat_name in sorted(stat_names):
        ref_id = to_letter_label(ref_int)
        stat_type = guess_stat_type(stat_name, stat_types)
        labels = dict()
        labels.update(filters or dict())
        labels.update(stat_labels.get(stat_name) or dict())
        name_filtered = f"{stat_name}{filter_string}"

        if "counter" == stat_type:
            name_filtered = f"rate({name_filtered}[$__rate_interval])"
        if labels:
            label_string = ", ".join(labels.keys())
            if "max" in name_filtered:
                stat_def = f"max by ({label_string}) ({name_filtered})"
            else:
                stat_def = f"sum by ({label_string}) ({name_filtered})"
        else:
            stat_def = f"{name_filtered}"
        stat_name = stat_name.removesuffix("_total")
        stat_name = stat_name.removesuffix("_count")
        stat_name = stat_name.removesuffix("_sum")
        dimensions = get_dimensions(labels)
        if dimensions:
            as_legend = f"{stat_name} {dimensions}"
        else:
            as_legend = stat_name
        target = grafana_entry(stat_def, as_legend, ref_id)
        targets.append(target)
        stat_defs[stat_name] = stat_def
        ref_int += 1
    for summary in summaries:
        sum_name = f"{summary}_sum"
        ref_id = to_letter_label(ref_int)
        seconds_def = stat_defs.get(sum_name)
        count_def = stat_defs.get(f"{summary}_count")
        labels = dict()
        labels.update(filters or dict())
        labels.update(stat_labels.get(sum_name) or dict())
        if seconds_def is not None and count_def is not None:
            dimensions = get_dimensions(labels)
            if dimensions:
                as_legend = f"{summary} {dimensions}"
            else:
                as_legend = summary
            expression = f"{seconds_def} / {count_def}"
            ref_int += 1
            target = grafana_entry(expression, as_legend, ref_id)
            targets.append(target)

    return json.dumps(targets, indent=4)


def to_letter_label(n: int) -> str:
    label = ""
    while n >= 0:
        label = chr(n % 26 + ord('A')) + label
        n = n // 26 - 1
    return label



def grafana_entry(
    expression: str,
    legend_format: str,
    ref_id: str
):
    return {
        "datasource": {
            "type": "prometheus",
            "uid": "-O-XQmcGz"
        },
        "editorMode": "code",
        "expr": f"{expression} != 0",
        "hide": False,
        "instant": False,
        "interval": "1m",
        "legendFormat": legend_format,
        "range": True,
        "refId": ref_id
    }



class NullWriter(TextIO):
    def write(self, s):
        pass

    def flush(self):
        pass


class Main(cmd.PrintingFileProcessor):
    def __init__(self):
        super().__init__()
        self.parser.add_argument(
            "--name",
            dest="name",
            default=None,
            required=False,
            help="what to call a top-level item",
        )
        self.parser.add_argument(
            "--matching",
            dest="matching",
            default=None,
            required=False,
            help="Only include items that match this word",
        )
        self.parser.add_argument(
            "--filters",
            metavar="FILTERS",
            type=str,
            nargs="*",
            help="""Any filters to add to every query: '{env="$env",project="foo"}' "bar=baz" a=b,c=d""",
        )
        self.parser.add_argument(
            "--as-grafana-panels",
            dest="as_grafana",
            action="store_true",
            default=False,
            help="Output JSON appropriate to insert into panels"
        )
        self.parser.add_argument(
            "--grafana-first-letter",
            dest="grafana_first_letter",
            default="A",
            help="If making json for grafana, start with this letter"

        )

    def setup(self):
        super().setup()
        if self.args.quiet:
            easy_logging.easy_initialize_logging("DEBUG", stream=NullWriter())
        elif self.args.verbose:
            easy_logging.easy_initialize_logging("DEBUG", stream=sys.stderr)
        else:
            easy_logging.easy_initialize_logging("INFO", stream=sys.stderr)

    def process_contents(self, contents: str) -> str:
        raw_filters = self.args.filters
        if self.args.matching is not None:
            contents =  "\n".join(line for line in contents.split("\n") if self.args.matching in line)
        return prometheus_to_queries(contents, parse_raw_filters(raw_filters), self.args.as_grafana, self.args.grafana_first_letter)


if __name__ == "__main__":
    Main().main()
